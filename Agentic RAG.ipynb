{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77df5c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, MessagesState,START,END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, Literal,TypedDict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "327f8cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm ready to help you.  Please tell me what you need.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.3)\n",
    "    response = llm.invoke(\"Are you ready to help me\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c4a1e",
   "metadata": {},
   "source": [
    "## Document Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0d027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e65d4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/cover_letter.pdf'\n",
    "loader = PyPDFLoader(file_path,mode='single')\n",
    "docs=loader.load()\n",
    "spliter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0,separator=\"\\n\")\n",
    "texts = spliter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "608e19a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Cover letter for apexanalytix', 'source': 'data/cover_letter.pdf', 'total_pages': 2}, page_content=\"Natdanai  Intraraksa  \\nAI  engineer  /  Data  Scientist   3217  w  Bertaue  Av.  Chicago,  IL  60618  \\n Intraraksa@gmail.com  \\n773-876-2897\\n \\n June  29,  2025  \\nHiring  manager  \\nApexanalytix  .Inc  \\n I  am  writing  to  express  my  strong  interest  in  the  AI  Engineer  \\nposition.\\n \\nWith\\n \\nover\\n \\nfive\\n \\nyears\\n \\nof\\n \\nexperience\\n \\nin\\n \\nAI\\n \\nengineering\\n \\nand\\n \\ndata\\n \\nscience,\\n \\nalong\\n \\nwith\\n \\na\\n \\ndecade\\n \\nof\\n \\nexperience\\n \\nas\\n \\na\\n \\nmanufacturing\\n \\nengineer,\\n \\nI\\n \\nbring\\n \\na\\n \\nwell-rounded\\n \\nbackground\\n \\nin\\n \\nboth\\n \\npractical\\n \\nengineering\\n \\nand\\n \\ncutting-edge\\n \\nAI\\n \\nsolution\\n \\ndevelopment.\\n \\nI\\n \\nrecently\\n \\nrelocated\\n \\nfrom\\n \\nThailand\\n \\nand\\n \\nam\\n \\neager\\n \\nto\\n \\ncontribute\\n \\nmy\\n \\nexpertise\\n \\nto\\n \\nyour\\n \\nteam.\\n \\n In  my  most  recent  role  at  Invitrace,  I  led  a  team  in  the  successful  \\ndeployment\\n \\nof\\n \\na\\n \\ngenerative\\n \\nAI\\n \\nsolution\\n \\nfor\\n \\none\\n \\nof\\n \\nThailand's\\n \\nlargest\\n \\nhospital\\n \\nnetworks.\\n \\nWe\\n \\nfine-tuned\\n \\nlarge\\n \\nlanguage\\n \\nmodels\\n \\nusing\\n \\nthe\\n \\nUnsloth\\n \\nframework\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Cover letter for apexanalytix', 'source': 'data/cover_letter.pdf', 'total_pages': 2}, page_content='to\\n \\ngenerate\\n \\nstructured\\n \\nmedical\\n \\ndocumentation\\n \\nfrom\\n \\nunstructured\\n \\nnotes,\\n \\nsignificantly\\n \\nreducing\\n \\nthe\\n \\nworkload\\n \\nfor\\n \\nhealthcare\\n \\nprofessionals.\\n \\nThis\\n \\nsolution\\n \\nintegrated\\n \\nretrieval-augmented\\n \\ngeneration\\n \\n(RAG)\\n \\nto\\n \\nprovide\\n \\nreal-time,\\n \\ncontextually\\n \\nrelevant\\n \\ncontent\\n \\nand\\n \\nimprove\\n \\naccuracy.\\n \\n Previously  at  iBotnoi,  I  collaborated  with  Phramongkutklao  and  \\nSamutprakarn\\n \\nhospitals\\n \\nto\\n \\ndevelop\\n \\nAI-powered\\n \\nvirtual\\n \\nassistants\\n \\n(Nurse\\n \\nAI).\\n \\nThese\\n \\nassistants\\n \\nused\\n \\nfunction-calling\\n \\ncapabilities\\n \\nand\\n \\nintegrated\\n \\nwith\\n \\nhospital\\n \\nAPIs\\n \\nto\\n \\nsupport\\n \\nappointment\\n \\nmanagement\\n \\nand\\n \\npatient\\n \\ninteraction\\n \\nworkflows.\\n \\nMy\\n \\nwork\\n \\non\\n \\nthese\\n \\nprojects\\n \\nincluded\\n \\nimplementing\\n \\nmulti-agent\\n \\nsystems\\n \\nusing\\n \\nframeworks\\n \\nlike\\n \\nLangGraph\\n \\nand\\n \\nCrewAI\\n \\nto\\n \\nmanage\\n \\nautonomous\\n \\ntask\\n \\ndelegation\\n \\nand\\n \\nexecution\\n \\n \\n I  have  hands-on  experience  fine-tuning  LLMs  with  techniques  such  as  \\nLoRA,\\n \\nQLoRA,\\n \\nand'),\n",
       " Document(metadata={'producer': 'Skia/PDF m140 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Cover letter for apexanalytix', 'source': 'data/cover_letter.pdf', 'total_pages': 2}, page_content='instruction\\n \\ntuning,\\n \\nand\\n \\nhave\\n \\nworked\\n \\nextensively\\n \\nwith\\n \\nLangChain,\\n \\nvector\\n \\ndatabases,\\n \\nand\\n \\nscalable\\n \\ndeployment\\n \\ntools\\n \\nsuch\\n \\nas\\n \\nDocker\\n \\nand\\n \\nKubernetes.\\n \\nMy\\n \\nbackground\\n \\nalso\\n \\nincludes\\n \\nbuilding\\n \\nMLOps\\n \\npipelines,\\n \\noptimizing\\n \\nmodel\\n \\nperformance,\\n \\nand\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nprivacy\\n \\nand\\n \\nsecurity\\n \\nstandards.\\n \\n I  would  welcome  the  opportunity  to  discuss  how  my  background  in  \\ndriving\\n \\nAI\\n \\nimplementation\\n \\nacross\\n \\ndiverse\\n \\nsectors\\n \\ncan\\n \\ncontribute\\n \\nto\\n \\nyour\\n \\nAI\\n \\nsolution.\\n \\nThank\\n \\nyou\\n \\nfor\\n \\nconsidering\\n \\nmy\\n \\napplication.\\n\\x0cSincerely,  \\nNatdanai  Intraraksa')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "829f4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a76d6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33d7e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0,separator=\"\\n\\n\")\n",
    "# texts = spliter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0,is_separator_regex=True, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72eaee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426f488",
   "metadata": {},
   "source": [
    "## create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e357a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/gemini-embedding-001')\n",
    "vectorstore = Chroma.from_documents(texts,embedding=embeddings,collection_name=\"sample_collection\",persist_directory=\"data/chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbc197",
   "metadata": {},
   "source": [
    "## Create retrival tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b9cedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    name=\"retriever_tool\",\n",
    "    description=\"This tool is used to retrieve relevant documents from the vector store based on the user's query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2511d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = retriver_tool.invoke({\"query\":\"Which technic that I use to fine-tune the model?\"})\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ebd6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grade Node\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        llm.bind_tools([retriver_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24b456c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ''' Query relevant documents from the vector store based on the user's query.'''\n",
    "\n",
    "    agent = create_react_agent(llm ,tools=[retriver_tool], prompt=prompt)\n",
    "    response = agent.invoke({'messages':state[\"messages\"]})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90884efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = MessagesState()\n",
    "state['messages'] = \"Which framework that I use to build the agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b036fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_query_or_respond(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f9d08b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The provided context only includes a `retriever_tool` function, which is a component that could be used within a larger agent framework.  There's no information about a complete agent framework.  To build an agent, you would need to choose a framework yourself, such as LangChain or similar.  The `retriever_tool` could then be integrated as a component within that framework.\n"
     ]
    }
   ],
   "source": [
    "a['messages'][0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b902055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-postinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
